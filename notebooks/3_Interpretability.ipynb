{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\odyky\\AppData\\Local\\Programs\\Python\\Python38\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/odyky/AppData/Local/Programs/Python/Python38/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import vgg16\n",
    "from sklearn.manifold import TSNE\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Experiment configuration\n",
    "data_augmentation = True\n",
    "batch_size_var = 512\n",
    "batch_norm_var = True\n",
    "l2 = True\n",
    "scheduler_bool = True\n",
    "early_stop = True\n",
    "\n",
    "\n",
    "# number of steps for early stopping\n",
    "early_stop_thresh = 3\n",
    "# number of epochs\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR10(root='~/.pytorch/CIFAR10', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='~/.pytorch/CIFAR10', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size_var, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size_var, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new dataset with augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = datasets.CIFAR10(root='~/.pytorch/CIFAR10', train=True, download=True, transform=augmentation_transform)\n",
    "\n",
    "# Concatenate the original trainset with the augmented dataset\n",
    "combined_trainset = ConcatDataset([trainset, augmented_dataset])\n",
    "\n",
    "# Create DataLoader for the combined training set only if we want it\n",
    "if data_augmentation:\n",
    "    train_loader = DataLoader(combined_trainset, batch_size=batch_size_var, shuffle=True)\n",
    "    print(\"Using data augmentation\")\n",
    "else:\n",
    "    print(\"No data augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.size(), labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sizes of the original trainset and the augmented combined trainset\n",
    "original_trainset_size = len(trainset)\n",
    "augmented_trainset_size = len(augmented_dataset)\n",
    "combined_trainset_size = len(combined_trainset)\n",
    "\n",
    "print(f\"Original Trainset Size: {original_trainset_size}\")\n",
    "print(f\"Augmented Trainset Size: {augmented_trainset_size}\")\n",
    "print(f\"Combined Trainset Size: {combined_trainset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Transformation for visualization (undo normalization)\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-mean[0] / std[0], -mean[1] / std[1], -mean[2] / std[2]],\n",
    "    std=[1 / std[0], 1 / std[1], 1 / std[2]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\odyky\\AppData\\Local\\Programs\\Python\\Python38\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/odyky/AppData/Local/Programs/Python/Python38/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Function to display an image\n",
    "def show_image(img, title):\n",
    "    img = inv_normalize(img)\n",
    "    img = torch.clamp(img, 0, 1)  # Clip values to stay within the valid range\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def checkpoint(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "# Function to visualize filters\n",
    "def visualize_filters(model, layer_idx):\n",
    "    layer = model[layer_idx]\n",
    "    filters = layer.weight.data.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(filters.shape[0]):\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(filters[i, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize activations\n",
    "def visualize_activations(model, layer_idx, input_image):\n",
    "    model = model.eval()\n",
    "    activation = None\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        nonlocal activation\n",
    "        activation = output.data.cpu().numpy()\n",
    "    \n",
    "    layer = model[layer_idx]\n",
    "    hook_handle = layer.register_forward_hook(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(input_image.unsqueeze(0).to(device))\n",
    "    \n",
    "    hook_handle.remove()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(activation.shape[1]):\n",
    "        plt.subplot(8, 8, i + 1)\n",
    "        plt.imshow(activation[0, i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Function to get top-k samples per unit\n",
    "def get_top_k_samples(model, layer_idx, k, data_loader):\n",
    "    model = model.eval()\n",
    "    activations = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        activations.append(output.data.cpu().numpy())\n",
    "    \n",
    "    layer = model[layer_idx]\n",
    "    hook_handle = layer.register_forward_hook(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            model(inputs)\n",
    "    \n",
    "    hook_handle.remove()\n",
    "    \n",
    "    activations = np.concatenate(activations, axis=0)\n",
    "    top_k_samples = np.argsort(activations, axis=0)[-k:]\n",
    "    \n",
    "    return top_k_samples\n",
    "\n",
    "# Function to show t-SNE plot\n",
    "def show_tsne_plot(model, layer_idx, data_loader):\n",
    "    model = model.eval()\n",
    "    features = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        features.append(output.data.cpu().numpy())\n",
    "    \n",
    "    layer = model[layer_idx]\n",
    "    hook_handle = layer.register_forward_hook(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            model(inputs)\n",
    "    \n",
    "    hook_handle.remove()\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    tsne = TSNE(n_components=2).fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(tsne[:, 0], tsne[:, 1])\n",
    "    plt.title(f't-SNE Plot - Layer {layer_idx}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first image from the batch\n",
    "show_image(images[0], title=f\"Label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNNCifar(nn.Module):\n",
    "    def __init__(self, num_classes=10, batch_norm=True):\n",
    "        super(CustomCNNCifar, self).__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=2, padding=1, padding_mode='zeros')\n",
    "        # 16x16x8\n",
    "        if self.batch_norm:\n",
    "            self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1, padding_mode='zeros')\n",
    "        # 8x8x16\n",
    "        if self.batch_norm:\n",
    "            self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1, padding_mode='zeros')\n",
    "        # 4x4x32\n",
    "        if self.batch_norm:\n",
    "            self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1, padding_mode='zeros')\n",
    "        # 2x2x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 1x1x64\n",
    "        if self.batch_norm:\n",
    "            self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.relu4 = nn.LeakyReLU()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)  # Softmax activation for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn4(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        x = self.log_softmax(x)  # Apply softmax for classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleCNNReduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model of SimpleCNNReduced from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = CustomCNNCifar(num_classes=10, batch_norm=batch_norm_var)\n",
    "loaded_model.load_state_dict(torch.load(\"best_model.pkl\"))\n",
    "loaded_model = loaded_model.to(device)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize filters in the first convolutional layer of your custom CNN model\n",
    "visualize_filters(loaded_model, 0)\n",
    "\n",
    "# Visualize activations in the first convolutional layer of your custom CNN model for a sample image\n",
    "sample_image, _ = next(iter(test_loader))\n",
    "sample_image = sample_image.to(device)\n",
    "visualize_activations(loaded_model, 0, sample_image)\n",
    "\n",
    "# Get top-k samples per unit in the first convolutional layer of your custom CNN model\n",
    "top_k_samples_custom_model = get_top_k_samples(loaded_model, 0, 5, test_loader)\n",
    "print(top_k_samples_custom_model)\n",
    "\n",
    "# Show t-SNE plot on the first Fully Connected layer of your custom CNN model\n",
    "show_tsne_plot(loaded_model, 11, test_loader)  # Adjust the layer index based on your model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained VGG16 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vgg16_model = vgg16(pretrained=True).features\n",
    "vgg16_model = vgg16_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize filters in the first convolutional layer of VGG16\n",
    "visualize_filters(vgg16_model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activations in the first convolutional layer of VGG16 for a sample image\n",
    "sample_image, _ = next(iter(test_loader))\n",
    "sample_image = sample_image.to(device)\n",
    "visualize_activations(vgg16_model, 0, sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top-k samples per unit in the first convolutional layer of VGG16\n",
    "top_k_samples = get_top_k_samples(vgg16_model, 0, 5, test_loader)\n",
    "print(top_k_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show t-SNE plot on the first Fully Connected layer of VGG16\n",
    "show_tsne_plot(vgg16_model, 6, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!**Note**!: You may need to adjust the layer indices based on the architecture of your pre-trained VGG16 model. The layer indices provided in the code are just examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
