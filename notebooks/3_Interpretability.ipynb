{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and transform CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(batch_size, use_augmentation=False):\n",
    "    # Standard CIFAR10 transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Augmented CIFAR10 transforms\n",
    "    train_transform_augmented = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Load CIFAR10 dataset\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Load augmented CIFAR10 dataset\n",
    "    trainset_augmented = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform_augmented)\n",
    "\n",
    "    # Combine datasets\n",
    "    if use_augmentation:\n",
    "        trainset_augmented = ConcatDataset([trainset, trainset_augmented])\n",
    "\n",
    "    # Choose the appropriate trainset\n",
    "    trainset_to_use = trainset_augmented if use_augmentation else trainset\n",
    "\n",
    "    # Create data loaders\n",
    "    trainloader = torch.utils.data.DataLoader(trainset_to_use, batch_size=batch_size, shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader = create_data_loaders(64, use_augmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_filters(weights, filename):\n",
    "    N = int(np.ceil(np.sqrt(weights.shape[0])))\n",
    "    f, axarr = plt.subplots(N, N, figsize=(12, 12))\n",
    "    scaled = (weights - weights.min()) / (weights.max() - weights.min())  # Scale the weights for better plotting\n",
    "\n",
    "    p = 0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            # Empty plot white when out of kernels to display\n",
    "            if p >= scaled.shape[0]:\n",
    "                krnl = torch.ones((scaled.shape[2], scaled.shape[3], 3))\n",
    "            else:\n",
    "                if scaled.shape[1] == 1:\n",
    "                    krnl = scaled[p, :, :, :].permute(1, 2, 0)\n",
    "                    axarr[i, j].imshow(krnl, cmap=\"gray\")\n",
    "                elif scaled.shape[1] == 3:\n",
    "                    krnl = scaled[p, :, :, :].permute(1, 2, 0)\n",
    "                    axarr[i, j].imshow(krnl)\n",
    "                else:\n",
    "                    krnl = scaled[p, 0, :, :]\n",
    "                    axarr[i, j].imshow(krnl, cmap=\"gray\")\n",
    "            axarr[i, j].axis(\"off\")\n",
    "            p += 1\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the CustomCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, use_batch_norm=False, use_dropout=False):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_dropout = use_dropout\n",
    "        \n",
    "        # Define the first set of convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if self.use_batch_norm else nn.Identity()\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32) if self.use_batch_norm else nn.Identity()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25) if self.use_dropout else nn.Identity()\n",
    "\n",
    "        # Define the second set of convolutional layers\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64) if self.use_batch_norm else nn.Identity()\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64) if self.use_batch_norm else nn.Identity()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(0.25) if self.use_dropout else nn.Identity()\n",
    "\n",
    "        # Define the third set of convolutional layers\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128) if self.use_batch_norm else nn.Identity()\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(128) if self.use_batch_norm else nn.Identity()\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout(0.25) if self.use_dropout else nn.Identity()\n",
    "\n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
    "        self.dropout4 = nn.Dropout(0.25) if self.use_dropout else nn.Identity()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First set of layers\n",
    "        x = self.conv1(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout1(x)\n",
    "\n",
    "        # Second set of layers\n",
    "        x = self.conv3(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout2(x)\n",
    "\n",
    "        # Third set of layers\n",
    "        x = self.conv5(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout3(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        if self.use_dropout:\n",
    "            x = self.dropout4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the CustomCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = CustomCNN(use_batch_norm=True, use_dropout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load custom_cnn_model_best_task2 model weights from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"custom_cnn_model_best_task2.pth\"\n",
    "network.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the Visualization to Convolutional Layers of CustomCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [network.conv1, network.conv2, network.conv3, network.conv4, network.conv5, network.conv6]\n",
    "\n",
    "for i, conv_layer in enumerate(conv_layers):\n",
    "    print(f\"Filters of Conv Layer {i+1}:\")\n",
    "    filters = conv_layer.weight.data.clone().cpu()\n",
    "    filename = f\"conv_layer_{i+1}_filters.png\"  # Filename for saving the figure\n",
    "    display_filters(filters, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Extractor Model for the CustomCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationExtractor(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ActivationExtractor, self).__init__()\n",
    "        # Copy layers from the original model up to the flatten layer\n",
    "        self.features = nn.Sequential(\n",
    "            original_model.conv1,\n",
    "            original_model.bn1,\n",
    "            original_model.conv2,\n",
    "            original_model.bn2,\n",
    "            original_model.pool1,\n",
    "            original_model.dropout1,\n",
    "            original_model.conv3,\n",
    "            original_model.bn3,\n",
    "            original_model.conv4,\n",
    "            original_model.bn4,\n",
    "            original_model.pool2,\n",
    "            original_model.dropout2,\n",
    "            original_model.conv5,\n",
    "            original_model.bn5,\n",
    "            original_model.conv6,\n",
    "            original_model.bn6,\n",
    "            original_model.pool3,\n",
    "            original_model.dropout3,\n",
    "            original_model.flatten\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the extractor model\n",
    "extractor = ActivationExtractor(network)\n",
    "# Ensure the extractor model is on the same device as the inputs\n",
    "extractor.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "for data in testloader:\n",
    "    inputs, _ = data\n",
    "    inputs = inputs.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Detach the output from the computation graph before converting to numpy\n",
    "    extracted_features = extractor(inputs).detach().cpu().numpy()\n",
    "    activations.append(extracted_features)\n",
    "\n",
    "activations = np.concatenate(activations)\n",
    "\n",
    "# Check the shape of the extracted features\n",
    "print(\"Shape of extracted activations:\", activations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate a pre-trained VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "vgg16.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the Visualization to Convolutional Layers of VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the features module of VGG16\n",
    "for i, layer in enumerate(vgg16.features):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        print(f\"Filters of Conv Layer {i}:\")\n",
    "        filters = layer.weight.data.clone().cpu()\n",
    "        filename = f\"vgg16_conv_layer_{i}_filters.png\"  # Filename for saving the figure\n",
    "        display_filters(filters, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-block3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
