{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1146d4",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25285748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "hparams = {\n",
    "    'batch_size': 32,\n",
    "    'use_l2_reg': True,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'log_interval': 100,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e563889",
   "metadata": {},
   "source": [
    "## Define the CustomCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50e079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN Architecture with Batch Normalization, and Dropout options\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, use_batch_norm=False, use_dropout=False):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        # First set of convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32) if self.use_batch_norm else nn.Identity()\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32) if self.use_batch_norm else nn.Identity()\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25) if self.use_dropout else nn.Identity()\n",
    "\n",
    "        # Second set of convolutional layers\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64) if self.use_batch_norm else nn.Identity()\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64) if self.use_batch_norm else nn.Identity()\n",
    "        self.relu4 = nn.LeakyReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(0.25) if self.use_dropout else nn.Identity()\n",
    "\n",
    "        # Third set of convolutional layers\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128) if self.use_batch_norm else nn.Identity()\n",
    "        self.relu5 = nn.LeakyReLU()\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(128) if self.use_batch_norm else nn.Identity()\n",
    "        self.relu6 = nn.LeakyReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout(0.25) if self.use_dropout else nn.Identity()\n",
    "\n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 128)  # Adjust the size according to your input image dimensions\n",
    "        self.relu7 = nn.LeakyReLU()\n",
    "        self.dropout4 = nn.Dropout(0.25) if self.use_dropout else nn.Identity()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First set of layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x) if self.use_batch_norm else x\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x) if self.use_batch_norm else x\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x) if self.use_dropout else x\n",
    "\n",
    "        # Second set of layers\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x) if self.use_batch_norm else x\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x) if self.use_batch_norm else x\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x) if self.use_dropout else x\n",
    "\n",
    "        # Third set of layers\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x) if self.use_batch_norm else x\n",
    "        x = self.relu5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x) if self.use_batch_norm else x\n",
    "        x = self.relu6(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x) if self.use_dropout else x\n",
    "\n",
    "        # Flatten the output\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu7(x)\n",
    "        x = self.dropout4(x) if self.use_dropout else x\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e31a91",
   "metadata": {},
   "source": [
    "## Define training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b84eaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_predictions(predicted_batch, label_batch):\n",
    "    pred = predicted_batch.argmax(dim=1, keepdim=True)\n",
    "    return pred.eq(label_batch.view_as(pred)).sum().item()\n",
    "\n",
    "def train_epoch(epoch, train_loader, network, optimizer, criterion, hparams):\n",
    "    network.train()\n",
    "    device = hparams['device']\n",
    "    avg_loss = None\n",
    "    avg_weight = 0.1\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if avg_loss:\n",
    "            avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\n",
    "        else:\n",
    "            avg_loss = loss.item()\n",
    "        if batch_idx % hparams['log_interval'] == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    return avg_loss\n",
    "\n",
    "def test_epoch(test_loader, network, hparams):\n",
    "    network.eval()\n",
    "    device = hparams['device']\n",
    "    test_loss = 0\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = network(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            acc += correct_predictions(output, target)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * acc / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {acc}/{len(test_loader.dataset)} ({test_acc:.0f}%)\\n')\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def plot_and_save_results(tr_losses, te_losses, te_accs, experiment_num):\n",
    "    # Plotting Training and Testing Losses\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(tr_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(te_losses, label='Testing Loss', color='red')\n",
    "    plt.title(f'Training and Testing Loss Over Epochs - Experiment {experiment_num}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting Testing Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(te_accs, label='Testing Accuracy', color='green')\n",
    "    plt.title(f'Testing Accuracy Over Epochs - Experiment {experiment_num}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(f\"transfer_{experiment_num}_results.png\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Function to run an experiment\n",
    "def run_training(trainloader, testloader, network, experiment_num=1, batch_size=hparams['batch_size'], use_l2_reg=False):\n",
    "    # Update hyperparameters based on the experiment\n",
    "    optimizer_weight_decay = 0.001 if use_l2_reg else 0\n",
    "    \n",
    "    # Calculate the total number of trainable parameters\n",
    "    total_params = count_parameters(network)\n",
    "    print(f\"Total trainable parameters in the model: {total_params}\")\n",
    "    print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "    # Define the optimizer with or without L2 regularization\n",
    "    optimizer = optim.Adam(network.parameters(), lr=hparams['learning_rate'], weight_decay=optimizer_weight_decay)\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training and validation loop\n",
    "    tr_losses, te_losses, te_accs = [], [], []\n",
    "    \n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, hparams['num_epochs'] + 1):\n",
    "        tr_losses.append(train_epoch(epoch, trainloader, network, optimizer, criterion, hparams))\n",
    "        te_loss, te_acc = test_epoch(testloader, network, hparams)\n",
    "        te_losses.append(te_loss)\n",
    "        te_accs.append(te_acc)\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time % 60)\n",
    "\n",
    "    print(f'Total Training Time: {elapsed_mins}m {elapsed_secs}s')\n",
    "    \n",
    "    plot_and_save_results(tr_losses, te_losses, te_accs, experiment_num)\n",
    "\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d9613",
   "metadata": {},
   "source": [
    "## Load Terrassa 900 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "921e855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerrassaDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        self.data = self._load_data()\n",
    "        self.class_to_idx = self._get_class_to_idx()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        annotation_file_path = os.path.join(self.root_dir, self.split, 'annotation.txt')\n",
    "\n",
    "        with open(annotation_file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                image_id, class_name = line.strip().split()\n",
    "                image_path = os.path.join(self.root_dir, self.split, 'images', f'{image_id}.jpg')\n",
    "                data.append((image_path, class_name))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _get_class_to_idx(self):\n",
    "        classes = set([item[1] for item in self.data])\n",
    "        class_to_idx = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "        return class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, class_name = self.data[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = torch.tensor(self.class_to_idx[class_name])\n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "# Set the path to your dataset\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go back one directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "\n",
    "# Combine with 'terrassa' to get the desired path\n",
    "dataset_root = os.path.join(parent_dir, 'terrassa')\n",
    "\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define transforms (you may customize this based on your needs)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "\n",
    "# Create train and test datasets\n",
    "train_dataset = TerrassaDataset(root_dir=dataset_root, split='train', transform=transform)\n",
    "test_dataset = TerrassaDataset(root_dir=dataset_root, split='test', transform=transform)\n",
    "val_dataset = TerrassaDataset(root_dir=dataset_root, split='val', transform=transform)\n",
    "\n",
    "# Concatenate the original trainset with the augmented dataset (70% of all data)\n",
    "combined_trainset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "terrassa_train_loader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "terrassa_test_loader = DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9b8f128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32]) torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "  print(images.size(), labels.size())\n",
    "  break\n",
    "    \n",
    "# Get a batch of data\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Transformation for visualization (undo normalization)\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-mean[0] / std[0], -mean[1] / std[1], -mean[2] / std[2]],\n",
    "    std=[1 / std[0], 1 / std[1], 1 / std[2]]\n",
    ")\n",
    "\n",
    "\n",
    "# Function to display an image\n",
    "def show_image(img, title):\n",
    "    img = inv_normalize(img)\n",
    "    img = torch.clamp(img, 0, 1)  # Clip values to stay within valid range\n",
    "    plt.imshow(img.permute(1, 2, 0))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display the first image from the batch\n",
    "show_image(images[0], title=f\"Label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b51527e",
   "metadata": {},
   "source": [
    "# 1. Custom CNN over Terrassa 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c65b72c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is B688-6E24\n",
      "\n",
      " Directory of C:\\Users\\pauco\\OneDrive\\Documentos\\DOCUMENTS\\JUPYTER\\block3_AML\\notebooks\n",
      "\n",
      "07/01/2024  11:31    <DIR>          .\n",
      "07/01/2024  11:06    <DIR>          ..\n",
      "07/01/2024  10:27    <DIR>          .ipynb_checkpoints\n",
      "07/01/2024  09:47           523.723 1_Baseline_CNN_CIFAR.ipynb\n",
      "07/01/2024  09:57           912.996 2_Experiments.ipynb\n",
      "07/01/2024  10:23         4.215.660 3_Interpretability.ipynb\n",
      "07/01/2024  11:31            15.187 4_Transfer learning.ipynb\n",
      "07/01/2024  09:44         2.223.323 custom_cnn_model.pth\n",
      "07/01/2024  09:44         2.223.323 custom_cnn_model_best_task2.pth\n",
      "07/01/2024  09:44    <DIR>          old_tries\n",
      "               6 File(s)     10.114.212 bytes\n",
      "               4 Dir(s)  339.045.879.808 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee78a9d",
   "metadata": {},
   "source": [
    "# 2. Custom CNN on CIFAR-10 as a feature extractor for Terrassa 900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25a2f3",
   "metadata": {},
   "source": [
    "### Load custom_cnn_model_best_task2 model weights from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52ddd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = CustomCNN(use_batch_norm=True, use_dropout=True)\n",
    "model_path = \"custom_cnn_model_best_task2.pth\"\n",
    "network.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7f876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
