{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score, fbeta_score\n",
    "\n",
    "colors = ['#648E9C', '#9CB1BC', '#C5D4DE', '#E8F1F4']\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot samples\n",
    "def plot_samples(images, N=5):\n",
    "    ps = random.sample(range(0, images.shape[0]), N**2)\n",
    "    f, axarr = plt.subplots(N, N, figsize=(10, 10))\n",
    "    p = 0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            im = images[ps[p]].transpose(1, 2, 0)\n",
    "            im = im / 2 + 0.5  # Unnormalize the image\n",
    "            axarr[i, j].imshow(im)\n",
    "            axarr[i, j].axis('off')\n",
    "            p += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_filters(weights, filename):\n",
    "    N = int(np.ceil(np.sqrt(weights.shape[0])))\n",
    "    f, axarr = plt.subplots(N, N, figsize=(12, 12))\n",
    "    scaled = (weights - weights.min()) / (weights.max() - weights.min())  # Scale the weights for better plotting\n",
    "\n",
    "    p = 0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            # Empty plot white when out of kernels to display\n",
    "            if p >= scaled.shape[0]:\n",
    "                krnl = torch.ones((scaled.shape[2], scaled.shape[3], 3))\n",
    "            else:\n",
    "                if scaled.shape[1] == 1:\n",
    "                    krnl = scaled[p, :, :, :].permute(1, 2, 0)\n",
    "                    axarr[i, j].imshow(krnl, cmap=\"gray\")\n",
    "                elif scaled.shape[1] == 3:\n",
    "                    krnl = scaled[p, :, :, :].permute(1, 2, 0)\n",
    "                    axarr[i, j].imshow(krnl)\n",
    "                else:\n",
    "                    krnl = scaled[p, 0, :, :]\n",
    "                    axarr[i, j].imshow(krnl, cmap=\"gray\")\n",
    "            axarr[i, j].axis(\"off\")\n",
    "            p += 1\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(filename)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_layers(network):\n",
    "    conv_layers = []\n",
    "    conv_layers_names = []\n",
    "    for name, module in network.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            conv_layers.append(module)\n",
    "            conv_layers_names.append(name)\n",
    "    return conv_layers, conv_layers_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_filters(model, model_folder, dt_str):\n",
    "    conv_result_folder = os.path.join(model_folder, f'conv_layers_{dt_str}')\n",
    "    os.makedirs(conv_result_folder, exist_ok=True)\n",
    "    conv_layers, _ = get_conv_layers(model)\n",
    "\n",
    "    # Now you can proceed with your existing code\n",
    "    for i, conv_layer in enumerate(conv_layers):\n",
    "        print(f\"Filters of Conv Layer {i+1}:\")\n",
    "        filters = conv_layer.weight.data.clone().cpu()\n",
    "        filename = os.path.join(conv_result_folder, f\"conv_layer_{i+1}_filters.png\")  # Filename for saving the figure\n",
    "        display_filters(filters, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function with unnormalization and saving the figure\n",
    "def vis_topk(ims, units, filename=\"top_k_activations.png\"):\n",
    "    f, axarr = plt.subplots(ims.shape[0], ims.shape[1], figsize=(16, 12))\n",
    "\n",
    "    for i in range(ims.shape[0]):\n",
    "        for j in range(ims.shape[1]):\n",
    "            axarr[i, j].imshow(ims[i, j, :, :, :])\n",
    "            axarr[i, j].axis('off')\n",
    "            if i == 0:  # Only set the title for the top row\n",
    "                axarr[i, j].set_title('Unit ' + str(units[j]))\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    # Save the figure\n",
    "    plt.savefig(filename)\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE plot on each of the Dense/Linear/Fully Connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(activations_tsne, labels, title='t-SNE visualization', figsize=(12, 10), point_size=20, cmap='tab10', filename='dense_layer_tsne.png'):\n",
    "    num_classes = len(np.unique(labels))\n",
    "    cifar_labels = [str(i) for i in range(num_classes)]\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    scatter = plt.scatter(activations_tsne[:, 0], activations_tsne[:, 1], c=labels, cmap=plt.get_cmap(cmap, num_classes), s=point_size, alpha=0.6)\n",
    "    plt.clim(-0.5, num_classes - 0.5)\n",
    "    cbar = plt.colorbar(scatter, ticks=range(num_classes))\n",
    "    cbar.ax.set_yticklabels(cifar_labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"t-SNE feature 1\")\n",
    "    plt.ylabel(\"t-SNE feature 2\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_predictions(predicted_batch, label_batch):\n",
    "    pred = predicted_batch.argmax(dim=1, keepdim=True)\n",
    "    return pred.eq(label_batch.view_as(pred)).sum().item()\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    return total_params, trainable_params, non_trainable_params\n",
    "\n",
    "def train_epoch(epoch, train_loader, network, optimizer, criterion, hparams, file_path):\n",
    "    network.train()\n",
    "    device = hparams['device']\n",
    "    network.to(device)\n",
    "    avg_loss = 0\n",
    "    with open(file_path, 'a') as file:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = network(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            if batch_idx % hparams['log_interval'] == 0:\n",
    "                log_str = (f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                           f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\\n')\n",
    "                file.write(log_str)\n",
    "                print(log_str, end='')  # print without adding an extra new line\n",
    "    return avg_loss / len(train_loader)\n",
    "\n",
    "def validation_epoch(validation_loader, network, hparams, file_path):\n",
    "    network.eval()\n",
    "    device = hparams['device']\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(), open(file_path, 'a') as file:\n",
    "        for data, target in validation_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = network(data)\n",
    "            validation_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            correct += correct_predictions(output, target)\n",
    "        validation_loss /= len(validation_loader.dataset)\n",
    "        test_acc = 100. * correct / len(validation_loader.dataset)\n",
    "        log_str = (f'\\nValidation set: Average loss: {validation_loss:.4f}, Accuracy: {correct}/{len(validation_loader.dataset)} '\n",
    "                   f'({test_acc:.0f}%)\\n')\n",
    "        file.write(log_str)\n",
    "        print(log_str, end='')  # print without adding an extra new line\n",
    "    return validation_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results, Training and Testing Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(tr_losses, val_losses, val_accs, filepath):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Training and Testing Losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(tr_losses, label='Training Loss', color='#1f77b4', linewidth=2, marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', color='#ff7f0e', linewidth=2, marker='x')\n",
    "    plt.title('Training and Validation Loss Over Epochs', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    # Testing Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accs, label='Validation Accuracy', color='green', linewidth=2, marker='^')\n",
    "    plt.title('Validation Accuracy Over Epochs', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gca().xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_true, y_preds, class_labels):\n",
    "    results = pd.DataFrame(precision_recall_fscore_support(y_true, y_preds)).T\n",
    "    results.rename(columns={0: 'Precision',\n",
    "                           1: 'Recall',\n",
    "                           2: 'F-Score',\n",
    "                           3: 'Support'}, inplace=True)\n",
    "    \n",
    "    conf_mat = pd.DataFrame(confusion_matrix(y_true, y_preds), \n",
    "                            columns=class_labels,\n",
    "                            index=class_labels)\n",
    "\n",
    "    f2 = fbeta_score(y_true, y_preds, beta=2, average='micro')\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Global F2 Score: {f2}\")\n",
    "    return results, conf_mat\n",
    "\n",
    "def plot_confusion_matrix(conf_mat, filepath):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap=sns.color_palette(\"icefire\", as_cmap=True), xticklabels=conf_mat.columns, yticklabels=conf_mat.index)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(images, y_true, y_preds, class_indices, filepath, num_samples=20):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for i, idx in enumerate(np.random.choice(len(images), size=num_samples, replace=False)):\n",
    "        ax = fig.add_subplot(4, 5, i + 1, xticks=[], yticks=[])\n",
    "        # Convert from PyTorch format (C, H, W) to image format (H, W, C)\n",
    "        ax.imshow(images[idx].permute(1, 2, 0).cpu().numpy())\n",
    "        pred_idx = y_preds[idx]\n",
    "        true_idx = y_true[idx]\n",
    "        \n",
    "        # Set title with predicted and true labels\n",
    "        ax.set_title(f\"{class_indices[pred_idx]}\\n(True: {class_indices[true_idx]})\", \n",
    "                     color=(\"green\" if pred_idx == true_idx else \"red\"))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting-up Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams = {\n",
    "#     'batch_size': 256, # from https://arxiv.org/pdf/1512.03385.pdf\n",
    "#     'num_epochs': 15,\n",
    "#     'learning_rate':  0.1, # from https://arxiv.org/pdf/1512.03385.pdf\n",
    "#     'learning_rate_factor': 0.1, # from https://arxiv.org/pdf/1512.03385.pdf\n",
    "#     'weight_decay': 0.0001, # from https://arxiv.org/pdf/1512.03385.pdf\n",
    "#     'momentum':  0.9, # from https://arxiv.org/pdf/1512.03385.pdf # Still not using this need to be applied in ResNet50\n",
    "#     'use_dropout': False, # from https://arxiv.org/pdf/1512.03385.pdf # Still not using this need to be applied in ResNet50\n",
    "#     'log_interval': 10,\n",
    "#     'device': device,\n",
    "#     'num_classes': 10\n",
    "# }\n",
    "hparams = {\n",
    "    'batch_size': 512,\n",
    "    'num_epochs': 15,\n",
    "    'learning_rate':  0.001,\n",
    "    'learning_rate_factor': 0.5,\n",
    "    'weight_decay': 0.001,\n",
    "    'momentum':  0.9,\n",
    "    'use_dropout': False,\n",
    "    'log_interval': 10,\n",
    "    'device': device,\n",
    "    'num_classes': 10\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Overview of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"./data/EuroSAT_RGB\"\n",
    "\n",
    "LABELS = os.listdir(DATASET)\n",
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot class distributions of whole dataset\n",
    "counts = {}\n",
    "\n",
    "for l in LABELS:\n",
    "    counts[l] = len(os.listdir(os.path.join(DATASET, l)))\n",
    "\n",
    "    \n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.bar(range(len(counts)), list(counts.values()), align='center', color= colors[1])\n",
    "plt.xticks(range(len(counts)), list(counts.keys()), fontsize=12, rotation=90)\n",
    "plt.xlabel('Class Label', fontsize=13)\n",
    "plt.ylabel('Class Size', fontsize=13)\n",
    "plt.title('EUROSAT Class Distribution', fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset Size: {sum(counts.values())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is split into 10 classes of land cover. Each class varies in size, so I'll have to stratify later on when splitting the data into training, testing and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [os.path.join(DATASET, l, l+'_1000.jpg') for l in LABELS]\n",
    "\n",
    "img_paths = img_paths + [os.path.join(DATASET, l, l+'_2000.jpg') for l in LABELS]\n",
    "\n",
    "def plot_sat_imgs(paths):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(20):\n",
    "        plt.subplot(4, 5, i+1, xticks=[], yticks=[])\n",
    "        img = PIL.Image.open(paths[i], 'r')\n",
    "        plt.imshow(np.asarray(img))\n",
    "        plt.title(paths[i].split('/')[-2])\n",
    "\n",
    "plot_sat_imgs(img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon reviewing the various class previews, we observe certain commonalities and distinct contrasts among them.\n",
    "\n",
    "Classes depicting urban settings like `Highways`, `Residential` areas, and `Industrial` sites predominantly feature man-made structures and roadways.\n",
    "\n",
    "Both `AnnualCrops` and `PermanentCrops` classes display agricultural areas, characterized by straight lines marking different crop fields.\n",
    "\n",
    "In contrast, classes such as `HerbaceousVegetation`, `Pasture`, and `Forests` represent natural landscapes. `Rivers`, also a part of the natural landscape category, are somewhat more distinguishable from these other natural classes.\n",
    "\n",
    "Analyzing the imagery content might give insights into potential class confusions. For instance, a `River` image could be misidentified as a `Highway`. Similarly, an image showing a highway junction with nearby buildings might be confused for an `Industrial` area. To overcome these challenges, it's crucial to develop a classifier that can adeptly discern these subtle differences.\n",
    "\n",
    "Regarding Sentinel-2 satellite imagery, it's possible to access over 10 additional spectral bands. For instance, the `Near-Infrared Radiation (NIR)` band is available in this dataset and can be utilized to create indices that visualize the presence or absence of radiation in an image. However, this dataset lacks NIR wavelength bands, rendering this approach infeasible for our current analysis. Nonetheless, it's important to note that NIR data could offer an alternative method for tackling this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train, Validation and Test Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = './data/train'\n",
    "VALID_DIR = './data/valid'\n",
    "TEST_DIR = './data/test'\n",
    "\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(VALID_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_DIR, exist_ok=True)\n",
    "\n",
    "# And open_project folder\n",
    "os.makedirs(f'../open_project/', exist_ok=True)\n",
    "\n",
    "# create class label subdirectories in train and test\n",
    "for l in LABELS:\n",
    "    os.makedirs(os.path.join(TRAIN_DIR, l), exist_ok=True)\n",
    "    os.makedirs(os.path.join(VALID_DIR, l), exist_ok=True)\n",
    "    os.makedirs(os.path.join(TEST_DIR, l), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with image paths and labels\n",
    "data = {os.path.join(DATASET, l, img): l for l in LABELS for img in os.listdir(os.path.join(DATASET, l))}\n",
    "\n",
    "X = pd.Series(list(data.keys()))\n",
    "y = pd.get_dummies(pd.Series(data.values()))\n",
    "\n",
    "# First split: Splitting into training and temporary set (combining test and validation)\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=69)  # 30% for test + validation\n",
    "\n",
    "for train_idx, temp_idx in split.split(X, y):\n",
    "    train_paths = X[train_idx].tolist()\n",
    "    temp_paths = X[temp_idx].tolist()\n",
    "\n",
    "# Second split: Splitting the temporary set into test and validation\n",
    "split_temp = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=69)  # 50% of 30% -> 15% each for test and validation\n",
    "y_temp = y.iloc[temp_idx]\n",
    "\n",
    "for test_idx, val_idx in split_temp.split(pd.Series(temp_paths), y_temp):\n",
    "    test_paths = pd.Series(temp_paths).iloc[test_idx].tolist()\n",
    "    val_paths = pd.Series(temp_paths).iloc[val_idx].tolist()\n",
    "\n",
    "# Define new paths without using regex\n",
    "new_train_paths = [path.replace(DATASET, TRAIN_DIR) for path in train_paths]\n",
    "new_test_paths = [path.replace(DATASET, TEST_DIR) for path in test_paths]\n",
    "new_val_paths = [path.replace(DATASET, VALID_DIR) for path in val_paths]\n",
    "\n",
    "train_path_map = zip(train_paths, new_train_paths)\n",
    "test_path_map = zip(test_paths, new_test_paths)\n",
    "val_path_map = zip(val_paths, new_val_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment Code Below when Run for First Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the files\n",
    "# print(\"Moving training files..\")\n",
    "# for old_path, new_path in tqdm(train_path_map):\n",
    "#     os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "#     shutil.copy(old_path, new_path)\n",
    "\n",
    "# print(\"Moving testing files..\")\n",
    "# for old_path, new_path in tqdm(test_path_map):\n",
    "#     os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "#     shutil.copy(old_path, new_path)\n",
    "\n",
    "# print(\"Moving validation files..\")\n",
    "# for old_path, new_path in tqdm(val_path_map):\n",
    "#     os.makedirs(os.path.dirname(new_path), exist_ok=True)\n",
    "#     shutil.copy(old_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing Train, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training\n",
    "mean = [0.485, 0.456, 0.406] # Adjust as needed\n",
    "std = [0.229, 0.224, 0.225] # Adjust as needed\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(60),\n",
    "    transforms.RandomResizedCrop(64, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)  \n",
    "])\n",
    "\n",
    "# Define transformations for testing (only normalization)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)  \n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader for training\n",
    "train_dataset = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
    "\n",
    "# Optionally create the dataset and dataloader for validation\n",
    "valid_dataset = datasets.ImageFolder(root=VALID_DIR, transform=test_transforms)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=hparams['batch_size'], shuffle=False)\n",
    "\n",
    "# Create the dataset and dataloader for testing\n",
    "test_dataset = datasets.ImageFolder(root=TEST_DIR, transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=hparams['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train Dataset Size: {len(train_dataset)}')\n",
    "print(f'Validation Dataset Size: {len(valid_dataset)}')\n",
    "print(f'Test Dataset Size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Class indices dictionary\n",
    "class_indices = train_dataset.class_to_idx\n",
    "# Reverse the mapping to get indices to class names\n",
    "idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "print(class_indices)\n",
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the first batch of images from the trainloader\n",
    "first_batch_images, _ = next(iter(train_loader))\n",
    "first_image = first_batch_images[0].unsqueeze(0).to(hparams['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch_images_np = first_batch_images.numpy()\n",
    "\n",
    "# Plot samples from the first batch\n",
    "plot_samples(first_batch_images_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EUROSatCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EUROSatCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EUROSatCNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # First set of convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Second set of convolutional layers\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.LeakyReLU()\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.relu4 = nn.LeakyReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2) \n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Third set of convolutional layers\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.relu5 = nn.LeakyReLU()\n",
    "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(128)\n",
    "        self.relu6 = nn.LeakyReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # output size: 8x8\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "\n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 128)  # 8x8 is the size after pooling layers\n",
    "        self.relu7 = nn.LeakyReLU()\n",
    "        self.dropout4 = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(128, self.num_classes)  # Assuming 10 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First set of layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second set of layers\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Third set of layers\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu7(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Instatiating Specific Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(cnn_base, num_classes, file_path):\n",
    "    # Choose the base model and modify the classifier\n",
    "    if cnn_base in ['ResNet50', 'ResNet152']:\n",
    "        if cnn_base == 'ResNet50':\n",
    "            model = models.resnet50(pretrained=True)\n",
    "            # Freeze the parameters\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Replace the final fully connected layer\n",
    "            # Extend the classifier\n",
    "            num_ftrs = model.fc.in_features\n",
    "        elif cnn_base == 'ResNet152':\n",
    "            model = models.resnet152(pretrained=True)\n",
    "            # Freeze the parameters\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Replace the final fully connected layer\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet model base\")\n",
    "    \n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    elif cnn_base in ['VGG16', 'VGG19']:\n",
    "        if cnn_base == 'VGG16':\n",
    "            model = models.vgg16(pretrained=True)\n",
    "            # Freeze the parameters\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        elif cnn_base == 'VGG19':\n",
    "            model = models.vgg19(pretrained=True)\n",
    "            # Freeze the parameters\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported VGG model base\")\n",
    "        \n",
    "        # Modify and extend the classifier\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    elif cnn_base == \"EUROSatCNN\":\n",
    "        model = EUROSatCNN(num_classes)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported Model\")\n",
    "    \n",
    "    # Get the current datetime\n",
    "    dt = datetime.now()\n",
    "\n",
    "    # Format the datetime as a string in the specified format: year_month_day_hour_minute\n",
    "    str_dt = dt.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "    \n",
    "    file_path = os.path.join(file_path, f'architecture_{str_dt}.txt')\n",
    "    \n",
    "    total_params, trainable_params, non_trainable_params = count_parameters(model)\n",
    "    \n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write('-------------------------------------------------------------------\\n')\n",
    "        file.write(f'{(model)}\\n')\n",
    "        file.write('-------------------------------------------------------------------\\n')\n",
    "        file.write(f'Total parameters in the model: {total_params}\\n')\n",
    "        file.write(f'Trainable parameters in the model: {trainable_params}\\n')\n",
    "        file.write(f'NON-Trainable parameters in the model: {non_trainable_params}\\n')\n",
    "        file.write('-------------------------------------------------------------------\\n')\n",
    "        file.write('-------------------------------------------------------------------\\n')\n",
    "        file.write(str(hparams))\n",
    "        \n",
    "    \n",
    "    print('-------------------------------------------------------------------\\n')\n",
    "    print(f'{(model)}\\n')\n",
    "    print('-------------------------------------------------------------------\\n')\n",
    "    print(f'Total parameters in the model: {total_params}\\n')\n",
    "    print(f'Trainable parameters in the model: {trainable_params}\\n')\n",
    "    print(f'NON-Trainable parameters in the model: {non_trainable_params}\\n')\n",
    "    print('-------------------------------------------------------------------\\n')\n",
    "\n",
    "    return model, str_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_process(network, nework_name, model_folder, str_dt):\n",
    "    \n",
    "    train_log_path = os.path.join(model_folder, f'training_log_{str_dt}.txt')\n",
    "    losses_figure_path = os.path.join(model_folder, f'losses_{str_dt}_.png')\n",
    "    \n",
    "    # Initialize variables for Early Stopping and Model Checkpoint\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    patience = 10  # For early stopping\n",
    "    \n",
    "\n",
    "    # Define the optimizer with or without L2 regularization\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, network.parameters()), lr=hparams['learning_rate'], weight_decay=hparams['weight_decay'])\n",
    "\n",
    "    # Define the criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize ReduceLROnPlateau scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=hparams['learning_rate_factor'], patience=3, min_lr=0.001)\n",
    "\n",
    "    early_stopping_triggered = False\n",
    "\n",
    "\n",
    "    # Training and validation loop\n",
    "    tr_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "    # Start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, hparams['num_epochs'] + 1):\n",
    "        train_loss = train_epoch(epoch, train_loader, network, optimizer, criterion, hparams, train_log_path)\n",
    "        tr_losses.append(train_loss)\n",
    "        val_loss, val_acc = validation_epoch(valid_loader, network, hparams, train_log_path)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "    \n",
    "\n",
    "        # Model Checkpoint\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            model_path = os.path.join(model_folder, f'best_model_{str_dt}.pth')\n",
    "            torch.save(network.state_dict(), model_path)\n",
    "            with open(train_log_path, 'a') as file:\n",
    "                file.write(f\"Checkpoint saved at {model_path} \\n\")\n",
    "            print(f\"Checkpoint saved at {model_path} \\n\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Early Stopping\n",
    "        if epochs_no_improve == patience:\n",
    "            with open(train_log_path, 'a') as file:\n",
    "                file.write(\"Early stopping triggered. Exiting training loop. \\n\")\n",
    "            print(\"Early stopping triggered. Exiting training loop. \\n\")\n",
    "            early_stopping_triggered = True\n",
    "            break\n",
    "\n",
    "        # Learning Rate Scheduler Step\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "    if not early_stopping_triggered:\n",
    "        with open(train_log_path, 'a') as file:\n",
    "            file.write(\"Completed training for {} epochs \\n\".format(hparams['num_epochs']))\n",
    "        print(\"Completed training for {} epochs \\n\".format(hparams['num_epochs']))\n",
    "    \n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Convert elapsed time to minutes and seconds\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time % 60)\n",
    "\n",
    "    with open(train_log_path, 'a') as file:\n",
    "        file.write(f'Total Training Time: {elapsed_mins}m {elapsed_secs}s \\n')\n",
    "    print(f'Total Training Time: {elapsed_mins}m {elapsed_secs}s \\n')\n",
    "    \n",
    "    plot_and_save_results(tr_losses, val_losses, val_accs, losses_figure_path)\n",
    "    \n",
    "    return tr_losses, val_losses, val_accs, nework_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the respective `Model` which you want to train and test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder = '../open_project/ResNet50'\n",
    "# os.makedirs(model_folder, exist_ok=True)\n",
    "# ResNet50_model, str_dt = compile_model('ResNet50', hparams['num_classes'], model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_losses, val_losses, val_accs, network_name = training_process(ResNet50_model, \"ResNet50\", model_folder, str_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet50_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder = '../open_project/ResNet152'\n",
    "# os.makedirs(model_folder, exist_ok=True)\n",
    "# ResNet152_model, str_dt = compile_model('ResNet152', hparams['num_classes'], model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_losses, val_losses, val_accs, network_name = training_process(ResNet152_model, \"ResNet152\", model_folder, str_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ResNet152_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder = '../open_project/VGG16'\n",
    "# os.makedirs(model_folder, exist_ok=True)\n",
    "# VGG16_model, str_dt = compile_model('VGG16', hparams['num_classes'], model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_losses, val_losses, val_accs, network_name = training_process(VGG16_model, \"VGG16\", model_folder, str_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VGG16_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder = '../open_project/VGG19'\n",
    "# os.makedirs(model_folder, exist_ok=True)\n",
    "# VGG19_model, str_dt = compile_model('VGG19', hparams['num_classes'], model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_losses, val_losses, val_accs, network_name = training_process(VGG19_model, \"VGG19\", model_folder, str_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VGG19_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EUROSatCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder = '../open_project/EUROSatCNN'\n",
    "# os.makedirs(model_folder, exist_ok=True)\n",
    "# EUROSatCNN, str_dt = compile_model('EUROSatCNN', hparams['num_classes'], model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_losses, val_losses, val_accs, nework_name = training_process(EUROSatCNN, \"EUROSatCNN\", model_folder, str_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = EUROSatCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chunk can be used in case you want to load a specific model and check its `predictability power`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EUROSatCNN\n",
    "# model = EUROSatCNN(hparams['num_classes'])\n",
    "# model_folder = '../open_project/EUROSatCNN_no_fine_tune'\n",
    "# model_version = \"best_model_2024_01_09_00_49.pth\"\n",
    "\n",
    "\n",
    "# ResNet152 & 50\n",
    "# model_folder = '../open_project/ResNet152'\n",
    "# model_folder = '../open_project/ResNet50'\n",
    "# model_version = \"best_model_2024_01_10_21_17.pth\"\n",
    "\n",
    "# model = models.resnet152(pretrained=True)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, hparams['num_classes'])\n",
    "# model.fc = nn.Sequential(\n",
    "#             nn.Linear(num_ftrs, 4096),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.25),\n",
    "#             nn.Linear(4096, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.25),\n",
    "#             nn.Linear(1024, hparams['num_classes'])\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "# VGG16 & VGG19\n",
    "model_folder = '../open_project/VGG16'\n",
    "# model_folder = '../open_project/VGG19'\n",
    "model_version = \"best_model_2024_01_11_13_04.pth\"\n",
    "# model_version = \"best_model_2024_01_11_13_04.pth\"\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "# Freeze the parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# Modify and extend the classifier\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(1024, hparams['num_classes'])\n",
    ")\n",
    "\n",
    "\n",
    "# # Extracting the date and time part using string slicing\n",
    "str_dt = model_version[len(\"best_model_\"):-len(\".pth\")]\n",
    "model_path = os.path.join(model_folder, model_version)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch of images and their labels from the test_loader\n",
    "images, labels = next(iter(train_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Predict labels\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map predicted and true labels back to class names\n",
    "predicted_labels = [idx_to_class[idx] for idx in predicted.cpu().numpy()]\n",
    "true_labels = [idx_to_class[idx] for idx in labels.cpu().numpy()]\n",
    "\n",
    "filename = os.path.join(model_folder, f'plot_train_predictions_{str_dt}.png')\n",
    "\n",
    "# Visualize predictions\n",
    "plot_predictions(images, true_labels, predicted_labels, class_indices, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store true and predicted labels\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Predict labels for the entire test dataset\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Append predictions and labels to the lists\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can pass all_labels and all_predictions to the display_results function\n",
    "prf, conf_mat = display_results(all_labels, all_predictions, class_indices)\n",
    "\n",
    "\n",
    "filename = os.path.join(model_folder, f'prf_train_{str_dt}.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "prf.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to plot the confusion matrix\n",
    "filename = os.path.join(model_folder, f'train_cm_{str_dt}.png')\n",
    "plot_confusion_matrix(conf_mat, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch of images and their labels from the test_loader\n",
    "images, labels = next(iter(test_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# Predict labels\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map predicted and true labels back to class names\n",
    "predicted_labels = [idx_to_class[idx] for idx in predicted.cpu().numpy()]\n",
    "true_labels = [idx_to_class[idx] for idx in labels.cpu().numpy()]\n",
    "\n",
    "filename = os.path.join(model_folder, f'plot_test_predictions_{str_dt}.png')\n",
    "\n",
    "# Visualize predictions\n",
    "plot_predictions(images, true_labels, predicted_labels, class_indices, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store true and predicted labels\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Predict labels for the entire test dataset\n",
    "model.eval()\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Append predictions and labels to the lists\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can pass all_labels and all_predictions to the display_results function\n",
    "prf, conf_mat = display_results(all_labels, all_predictions, class_indices)\n",
    "\n",
    "\n",
    "filename = os.path.join(model_folder, f'prf_test_{str_dt}.csv')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "prf.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to plot the confusion matrix\n",
    "filename = os.path.join(model_folder, f'test_cm_{str_dt}.png')\n",
    "plot_confusion_matrix(conf_mat, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters (Convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_conv_filters(model, model_folder, str_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EUROSatCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationExtractorEuroSATCNN(nn.Module):\n",
    "    def __init__(self, original_model, stop_layer):\n",
    "        super(ActivationExtractorEuroSATCNN, self).__init__()\n",
    "        self.stop_layer = stop_layer\n",
    "        # Copy layers from the original model up to the flatten layer\n",
    "        self.features = nn.Sequential(\n",
    "            original_model.conv1,\n",
    "            original_model.bn1,\n",
    "            original_model.relu1,\n",
    "            original_model.conv2,\n",
    "            original_model.bn2,\n",
    "            original_model.relu2,\n",
    "            original_model.pool1,\n",
    "            original_model.dropout1,\n",
    "            original_model.conv3,\n",
    "            original_model.bn3,\n",
    "            original_model.relu3,\n",
    "            original_model.conv4,\n",
    "            original_model.bn4,\n",
    "            original_model.relu4,\n",
    "            original_model.pool2,\n",
    "            original_model.dropout2,\n",
    "            original_model.conv5,\n",
    "            original_model.bn5,\n",
    "            original_model.relu5,\n",
    "            original_model.conv6,\n",
    "            original_model.bn6,\n",
    "            original_model.relu6,\n",
    "            original_model.pool3,\n",
    "            original_model.dropout3,\n",
    "            original_model.flatten,\n",
    "            original_model.fc1\n",
    "        )\n",
    "        if stop_layer == 'fc1':\n",
    "            self.classifier = nn.Identity()\n",
    "        elif stop_layer == 'fc2':\n",
    "            self.classifier = nn.Sequential(\n",
    "                original_model.relu7,\n",
    "                original_model.dropout4,\n",
    "                original_model.fc2\n",
    "            )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the extractor model\n",
    "# Create extractors for each fully connected layer\n",
    "extractor_fc1 = ActivationExtractorEuroSATCNN(model, stop_layer='fc1')\n",
    "extractor_fc2 = ActivationExtractorEuroSATCNN(model, stop_layer='fc2')\n",
    "# Ensure the extractor model is on the same device as the inputs\n",
    "extractor_fc1.to(device)\n",
    "extractor_fc2.to(device)\n",
    "\n",
    "extractor_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable gradients computation\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_fc1 = []\n",
    "activations_fc2 = []\n",
    "for data in test_loader:\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    activations_fc1.append(extractor_fc1(inputs).cpu().numpy())\n",
    "    activations_fc2.append(extractor_fc2(inputs).cpu().numpy())\n",
    "\n",
    "activations_fc1 = np.concatenate(activations_fc1)\n",
    "activations_fc2 = np.concatenate(activations_fc2)\n",
    "\n",
    "# Check the shape of the extracted features\n",
    "print(\"Shape of extracted activations including dense layer 1:\", activations_fc1.shape)\n",
    "print(\"Shape of extracted activations including dense layer 2:\", activations_fc2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding per unit top K samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EUROSAT test dataset\n",
    "testimages = datasets.ImageFolder(root=TEST_DIR, transform=None)\n",
    "\n",
    "K = 10  # Number of top activations to retrieve\n",
    "idxs_top10_fc1 = np.argsort(activations_fc1, axis=0)[::-1][0:K, :]\n",
    "idxs_top10_fc2 = np.argsort(activations_fc2, axis=0)[::-1][0:K, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array to store the picked samples\n",
    "#picked_samples = np.zeros((K, n_units, 32, 32, 3), dtype=float)\n",
    "picked_samples_fc1 = np.zeros((K, idxs_top10_fc1.shape[1], 64, 64, 3), dtype=float)\n",
    "picked_samples_fc2 = np.zeros((K, idxs_top10_fc2.shape[1], 64, 64, 3), dtype=float)\n",
    "\n",
    "for i in range(idxs_top10_fc1.shape[0]):\n",
    "    for j in range(idxs_top10_fc1.shape[1]):\n",
    "        # Normalize the images from CIFAR10 as they are PIL Images\n",
    "        img_as_np = np.asarray(testimages[idxs_top10_fc1[i, j]][0]) / 255.0\n",
    "        picked_samples_fc1[i, j, :, :, :] = img_as_np\n",
    "\n",
    "for i in range(idxs_top10_fc2.shape[0]):\n",
    "    for j in range(idxs_top10_fc2.shape[1]):\n",
    "        # Normalize the images from CIFAR10 as they are PIL Images\n",
    "        img_as_np = np.asarray(testimages[idxs_top10_fc2[i, j]][0]) / 255.0\n",
    "        picked_samples_fc2[i, j, :, :, :] = img_as_np\n",
    "\n",
    "# Check the shape of picked_samples\n",
    "print(\"Shape of picked_samples including dense layer 1:\", picked_samples_fc1.shape)\n",
    "print(\"Shape of picked_samples including dense layer 2:\", picked_samples_fc2.shape)\n",
    "# The shape of the tensor corresponds to:\n",
    "# (n_images,n_units,nb_rows,nb_cols,nb_channels)\n",
    "# for each of the 2048 units in the layers, we have 10 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = np.rint(np.linspace(0, 127, num=20)).astype(int)\n",
    "nunits = len(units)\n",
    "# Slicing picked_samples to get the top K images for specified units\n",
    "# Adjust the indexing to correctly slice the array\n",
    "ims = picked_samples_fc1[:, units, :, :].squeeze()\n",
    "\n",
    "filename = os.path.join(model_folder, f'topk_fc1_{str_dt}.png')\n",
    "vis_topk(ims, units, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # Specified units\n",
    "nunits = len(units)\n",
    "# Slicing picked_samples to get the top K images for specified units\n",
    "# Adjust the indexing to correctly slice the array\n",
    "ims = picked_samples_fc2[:, units, :, :].squeeze()\n",
    "\n",
    "filename = os.path.join(model_folder, f'topk_fc2_{str_dt}.png')\n",
    "vis_topk(ims, units, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE plot on each of the Dense/Linear/Fully Connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA for dimensionality reduction before t-SNE\n",
    "t0 = time.time()  # Start timer\n",
    "\n",
    "# Perform t-SNE\n",
    "n_components_tsne = 2  # t-SNE components (2D for visualization)\n",
    "n_iter_tsne = 4000  # Number of iterations for t-SNE optimization\n",
    "tsne = TSNE(n_components=n_components_tsne, random_state=0, n_iter=n_iter_tsne)\n",
    "\n",
    "\n",
    "activations_fc1_tsne = tsne.fit_transform(activations_fc1)\n",
    "t1 = time.time()  # Stop timer\n",
    "\n",
    "print(f\"t-SNE done! Time elapsed: {t1 - t0:.2f} seconds\")\n",
    "print(\"Shape of t-SNE 2D features:\", activations_fc1_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the labels for test dataset\n",
    "labels = [y for _, y in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_folder, f't_SNE_fc1_{str_dt}.png')\n",
    "plot_tsne(activations_fc1_tsne, labels, title=\"t-SNE visualization of ResNet50 activation, Dense Layer 1\", filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA for dimensionality reduction before t-SNE\n",
    "t0 = time.time()  # Start timer\n",
    "\n",
    "# Perform t-SNE\n",
    "n_components_tsne = 2  # t-SNE components (2D for visualization)\n",
    "n_iter_tsne = 4000  # Number of iterations for t-SNE optimization\n",
    "tsne = TSNE(n_components=n_components_tsne, random_state=0, n_iter=n_iter_tsne)\n",
    "\n",
    "\n",
    "activations_fc2_tsne = tsne.fit_transform(activations_fc2)\n",
    "t1 = time.time()  # Stop timer\n",
    "\n",
    "print(f\"t-SNE done! Time elapsed: {t1 - t0:.2f} seconds\")\n",
    "print(\"Shape of t-SNE 2D features:\", activations_fc2_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_folder, f't_SNE_fc2_{str_dt}.png')\n",
    "plot_tsne(activations_fc1_tsne, labels, title=\"t-SNE visualization of ResNet50 activation, Output Layer\", filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 & ResNet152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ActivationExtractorResNet50_152(nn.Module):\n",
    "#     def __init__(self, original_model):\n",
    "#         super(ActivationExtractorResNet50_152, self).__init__()\n",
    "\n",
    "#         # Use all layers before the fully connected layers as features\n",
    "#         self.features = nn.Sequential(*list(original_model.children())[:-2])\n",
    "\n",
    "#         # Adaptive pooling to match the input size of the first linear layer\n",
    "#         self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "#         # Extracting only the first layer of the fully connected block\n",
    "#         self.fc = nn.Sequential(*list(original_model.fc.children())[:1])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = self.adaptive_pool(x)  # Adaptive pooling\n",
    "#         x = x.view(x.size(0), -1)  # Flatten the output\n",
    "#         x = self.fc(x)\n",
    "#         return x\n",
    "class ActivationExtractorResNet50_152(nn.Module):\n",
    "    def __init__(self, original_model, layer_num=1):\n",
    "        super(ActivationExtractorResNet50_152, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(*list(original_model.children())[:-2])\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        if layer_num == 1:\n",
    "            self.fc = nn.Sequential(*list(original_model.fc.children())[:1])\n",
    "        elif layer_num == 2:\n",
    "            self.fc = nn.Sequential(*list(original_model.fc.children())[:3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the extractor model\n",
    "# Create extractors for each fully connected layer\n",
    "\n",
    "# This command can be used for both 50 and 152.\n",
    "extractor_fc1 = ActivationExtractorResNet50_152(model, 1)\n",
    "extractor_fc2 = ActivationExtractorResNet50_152(model, 2)\n",
    "extractor = model\n",
    "\n",
    "# Disable gradients computation\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_fc1.to(device) # for fc1\n",
    "extractor_fc2.to(device) # for fc2\n",
    "extractor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_first_linear = []\n",
    "activations_second_linear = []\n",
    "activations_third_linear = []\n",
    "\n",
    "for data in test_loader:\n",
    "    inputs = data[0].to(device)\n",
    "    \n",
    "    # Extract and store the activations\n",
    "    activations_first_linear.append(extractor_fc1(inputs).cpu().numpy())\n",
    "    activations_second_linear.append(extractor_fc2(inputs).cpu().numpy())\n",
    "    activations_third_linear.append(extractor(inputs).cpu().numpy())\n",
    "\n",
    "# Concatenate the results from each extractor\n",
    "activations_first_linear = np.concatenate(activations_first_linear)\n",
    "activations_second_linear = np.concatenate(activations_second_linear)\n",
    "activations_third_linear = np.concatenate(activations_third_linear)\n",
    "\n",
    "# Check the shape of the extracted features\n",
    "print(\"Shape of extracted activations including first linear layer:\", activations_first_linear.shape)\n",
    "print(\"Shape of extracted activations including second linear layer:\", activations_second_linear.shape)\n",
    "print(\"Shape of extracted activations including third linear layer:\", activations_third_linear.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding per unit top K samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EUROSAT test dataset\n",
    "testimages = datasets.ImageFolder(root=TEST_DIR, transform=None)\n",
    "\n",
    "K = 10  # Number of top activations to retrieve\n",
    "\n",
    "# For the first fully connected layer\n",
    "idxs_top10_fc1 = np.argsort(activations_first_linear, axis=0)[::-1][0:K, :]\n",
    "\n",
    "# For the second fully connected layer\n",
    "idxs_top10_fc2 = np.argsort(activations_second_linear, axis=0)[::-1][0:K, :]\n",
    "\n",
    "# For the final fully connected layer\n",
    "idxs_top10_fc3 = np.argsort(activations_third_linear, axis=0)[::-1][0:K, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_top10_fc1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_top10_fc2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_top10_fc3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store the picked samples\n",
    "picked_samples_fc1 = np.zeros((K, idxs_top10_fc1.shape[1], 64, 64, 3), dtype=float)\n",
    "picked_samples_fc2 = np.zeros((K, idxs_top10_fc2.shape[1], 64, 64, 3), dtype=float)\n",
    "picked_samples_fc3 = np.zeros((K, idxs_top10_fc3.shape[1], 64, 64, 3), dtype=float)\n",
    "\n",
    "for i in range(K):\n",
    "    for j in range(idxs_top10_fc1.shape[1]):\n",
    "        # Normalize the images from CIFAR10 as they are PIL Images\n",
    "        img_as_np = np.asarray(testimages[idxs_top10_fc1[i, j]][0]) / 255.0\n",
    "        picked_samples_fc1[i, j, :, :, :] = img_as_np\n",
    "\n",
    "for i in range(K):\n",
    "    for j in range(idxs_top10_fc2.shape[1]):\n",
    "        # Normalize the images from CIFAR10 as they are PIL Images\n",
    "        img_as_np = np.asarray(testimages[idxs_top10_fc2[i, j]][0]) / 255.0\n",
    "        picked_samples_fc2[i, j, :, :, :] = img_as_np\n",
    "\n",
    "for i in range(K):\n",
    "    for j in range(idxs_top10_fc3.shape[1]):\n",
    "        # Normalize the images from CIFAR10 as they are PIL Images\n",
    "        img_as_np = np.asarray(testimages[idxs_top10_fc3[i, j]][0]) / 255.0\n",
    "        picked_samples_fc3[i, j, :, :, :] = img_as_np\n",
    "\n",
    "# Check the shape of picked_samples\n",
    "print(\"Shape of picked_samples including dense layer 1:\", picked_samples_fc1.shape)\n",
    "print(\"Shape of picked_samples including dense layer 2:\", picked_samples_fc2.shape)\n",
    "print(\"Shape of picked_samples including dense layer 3:\", picked_samples_fc3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = np.rint(np.linspace(0, 511, num=20)).astype(int)\n",
    "nunits = len(units)\n",
    "# Slicing picked_samples to get the top K images for specified units\n",
    "# Adjust the indexing to correctly slice the array\n",
    "ims = picked_samples_fc1[:, units, :, :].squeeze()\n",
    "filename = os.path.join(model_folder, f'topk_fc1_{str_dt}.png')\n",
    "vis_topk(ims, units, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = np.rint(np.linspace(0, 9, num=10)).astype(int)\n",
    "nunits = len(units)\n",
    "# Slicing picked_samples to get the top K images for specified units\n",
    "# Adjust the indexing to correctly slice the array\n",
    "ims = picked_samples_fc2[:, units, :, :].squeeze()\n",
    "filename = os.path.join(model_folder, f'topk_fc2_{str_dt}.png')\n",
    "vis_topk(ims, units, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = np.rint(np.linspace(0, 9, num=10)).astype(int)\n",
    "nunits = len(units)\n",
    "# Slicing picked_samples to get the top K images for specified units\n",
    "# Adjust the indexing to correctly slice the array\n",
    "ims = picked_samples_fc3[:, units, :, :].squeeze()\n",
    "filename = os.path.join(model_folder, f'topk_fc3_{str_dt}.png')\n",
    "vis_topk(ims, units, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE plot on each of the Dense/Linear/Fully Connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA for dimensionality reduction before t-SNE\n",
    "t0 = time.time()  # Start timer\n",
    "\n",
    "# Perform t-SNE\n",
    "n_components_tsne = 2  # t-SNE components (2D for visualization)\n",
    "n_iter_tsne = 4000  # Number of iterations for t-SNE optimization\n",
    "tsne = TSNE(n_components=n_components_tsne, random_state=0, n_iter=n_iter_tsne)\n",
    "\n",
    "\n",
    "activations_fc1_tsne = tsne.fit_transform(activations_first_linear)\n",
    "t1 = time.time()  # Stop timer\n",
    "\n",
    "print(f\"t-SNE done! Time elapsed: {t1 - t0:.2f} seconds\")\n",
    "print(\"Shape of t-SNE 2D features:\", activations_fc1_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the labels for test dataset\n",
    "labels = [y for _, y in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_folder, f't_SNE_fc1_{str_dt}.png')\n",
    "plot_tsne(activations_fc1_tsne, labels, title=\"t-SNE visualization of ResNet50 activation, Dense Layer 1\", filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA for dimensionality reduction before t-SNE\n",
    "t0 = time.time()  # Start timer\n",
    "\n",
    "# Perform t-SNE\n",
    "n_components_tsne = 2  # t-SNE components (2D for visualization)\n",
    "n_iter_tsne = 4000  # Number of iterations for t-SNE optimization\n",
    "tsne = TSNE(n_components=n_components_tsne, random_state=0, n_iter=n_iter_tsne)\n",
    "\n",
    "\n",
    "activations_fc2_tsne = tsne.fit_transform(activations_second_linear)\n",
    "t1 = time.time()  # Stop timer\n",
    "\n",
    "print(f\"t-SNE done! Time elapsed: {t1 - t0:.2f} seconds\")\n",
    "print(\"Shape of t-SNE 2D features:\", activations_fc2_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_folder, f't_SNE_fc2_{str_dt}.png')\n",
    "plot_tsne(activations_fc1_tsne, labels, title=\"t-SNE visualization of ResNet50 activation, Dense Layer 2\", filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA for dimensionality reduction before t-SNE\n",
    "t0 = time.time()  # Start timer\n",
    "\n",
    "# Perform t-SNE\n",
    "n_components_tsne = 2  # t-SNE components (2D for visualization)\n",
    "n_iter_tsne = 4000  # Number of iterations for t-SNE optimization\n",
    "tsne = TSNE(n_components=n_components_tsne, random_state=0, n_iter=n_iter_tsne)\n",
    "\n",
    "\n",
    "activations_fc3_tsne = tsne.fit_transform(activations_third_linear)\n",
    "t1 = time.time()  # Stop timer\n",
    "\n",
    "print(f\"t-SNE done! Time elapsed: {t1 - t0:.2f} seconds\")\n",
    "print(\"Shape of t-SNE 2D features:\", activations_fc3_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_folder, f't_SNE_fc3_{str_dt}.png')\n",
    "plot_tsne(activations_fc3_tsne, labels, title=\"t-SNE visualization of VGG16 activation, Output Layer\", filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 & VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationExtractorVGG(nn.Module):\n",
    "    def __init__(self, original_model, layer_num=1):\n",
    "        super(ActivationExtractorVGG, self).__init__()\n",
    "\n",
    "        self.features = original_model.features\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "        if layer_num == 1:\n",
    "            self.classifier = nn.Sequential(*list(original_model.classifier.children())[:1])\n",
    "        elif layer_num == 2:\n",
    "            self.classifier = nn.Sequential(*list(original_model.classifier.children())[:4])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the extractor model\n",
    "# Create extractors for each fully connected layer\n",
    "\n",
    "# This command can be used for both 50 and 152.\n",
    "extractor_fc1 = ActivationExtractorVGG(model, 1)\n",
    "extractor_fc2 = ActivationExtractorVGG(model, 2)\n",
    "extractor = model\n",
    "\n",
    "# Disable gradients computation\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_fc1.to(device) # for fc1\n",
    "# extractor_fc2.to(device) # for fc2\n",
    "# extractor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_first_linear = []\n",
    "# activations_second_linear = []\n",
    "# activations_third_linear = []\n",
    "\n",
    "for data in test_loader:\n",
    "    inputs = data[0].to(device)\n",
    "    \n",
    "    # Extract and store the activations\n",
    "    activations_first_linear.append(extractor_fc1(inputs).cpu().numpy())\n",
    "    # activations_second_linear.append(extractor_fc2(inputs).cpu().numpy())\n",
    "    # activations_third_linear.append(extractor(inputs).cpu().numpy())\n",
    "\n",
    "# Concatenate the results from each extractor\n",
    "activations_first_linear = np.concatenate(activations_first_linear)\n",
    "# activations_second_linear = np.concatenate(activations_second_linear)\n",
    "# activations_third_linear = np.concatenate(activations_third_linear)\n",
    "\n",
    "# Check the shape of the extracted features\n",
    "print(\"Shape of extracted activations including first linear layer:\", activations_first_linear.shape)\n",
    "# print(\"Shape of extracted activations including second linear layer:\", activations_second_linear.shape)\n",
    "# print(\"Shape of extracted activations including third linear layer:\", activations_third_linear.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding per unit top K samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EUROSAT test dataset\n",
    "testimages = datasets.ImageFolder(root=TEST_DIR, transform=None)\n",
    "\n",
    "K = 10  # Number of top activations to retrieve\n",
    "\n",
    "# For the first fully connected layer\n",
    "idxs_top10_fc1 = np.argsort(activations_first_linear, axis=0)[::-1][0:K, :]\n",
    "\n",
    "# For the second fully connected layer\n",
    "# idxs_top10_fc2 = np.argsort(activations_second_linear, axis=0)[::-1][0:K, :]\n",
    "\n",
    "# For the final fully connected layer\n",
    "# idxs_top10_fc3 = np.argsort(activations_third_linear, axis=0)[::-1][0:K, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_top10_fc1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_top10_fc2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_top10_fc3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store the picked samples\n",
    "picked_samples_fc1 = np.zeros((K, idxs_top10_fc1.shape[1], 64, 64, 3), dtype=float)\n",
    "# picked_samples_fc2 = np.zeros((K, idxs_top10_fc2.shape[1], 64, 64, 3), dtype=float)\n",
    "# picked_samples_fc3 = np.zeros((K, idxs_top10_fc3.shape[1], 64, 64, 3), dtype=float)\n",
    "\n",
    "for i in range(K):\n",
    "    for j in range(idxs_top10_fc1.shape[1]):\n",
    "        # Normalize the images from CIFAR10 as they are PIL Images\n",
    "        img_as_np = np.asarray(testimages[idxs_top10_fc1[i, j]][0]) / 255.0\n",
    "        picked_samples_fc1[i, j, :, :, :] = img_as_np\n",
    "\n",
    "# for i in range(K):\n",
    "#     for j in range(idxs_top10_fc2.shape[1]):\n",
    "#         # Normalize the images from CIFAR10 as they are PIL Images\n",
    "#         img_as_np = np.asarray(testimages[idxs_top10_fc2[i, j]][0]) / 255.0\n",
    "#         picked_samples_fc2[i, j, :, :, :] = img_as_np\n",
    "\n",
    "# for i in range(K):\n",
    "#     for j in range(idxs_top10_fc3.shape[1]):\n",
    "#         # Normalize the images from CIFAR10 as they are PIL Images\n",
    "#         img_as_np = np.asarray(testimages[idxs_top10_fc3[i, j]][0]) / 255.0\n",
    "#         picked_samples_fc3[i, j, :, :, :] = img_as_np\n",
    "\n",
    "# Check the shape of picked_samples\n",
    "print(\"Shape of picked_samples including dense layer 1:\", picked_samples_fc1.shape)\n",
    "# print(\"Shape of picked_samples including dense layer 2:\", picked_samples_fc2.shape)\n",
    "# print(\"Shape of picked_samples including dense layer 3:\", picked_samples_fc3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = np.rint(np.linspace(0, 511, num=20)).astype(int)\n",
    "nunits = len(units)\n",
    "# Slicing picked_samples to get the top K images for specified units\n",
    "# Adjust the indexing to correctly slice the array\n",
    "ims = picked_samples_fc1[:, units, :, :].squeeze()\n",
    "filename = os.path.join(model_folder, f'topk_fc1_{str_dt}.png')\n",
    "vis_topk(ims, units, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = np.rint(np.linspace(0, 9, num=10)).astype(int)\n",
    "nunits = len(units)\n",
    "# Slicing picked_samples to get the top K images for specified units\n",
    "# Adjust the indexing to correctly slice the array\n",
    "ims = picked_samples_fc2[:, units, :, :].squeeze()\n",
    "filename = os.path.join(model_folder, f'topk_fc2_{str_dt}.png')\n",
    "vis_topk(ims, units, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = np.rint(np.linspace(0, 9, num=10)).astype(int)\n",
    "nunits = len(units)\n",
    "# Slicing picked_samples to get the top K images for specified units\n",
    "# Adjust the indexing to correctly slice the array\n",
    "ims = picked_samples_fc3[:, units, :, :].squeeze()\n",
    "filename = os.path.join(model_folder, f'topk_fc3_{str_dt}.png')\n",
    "vis_topk(ims, units, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE plot on each of the Dense/Linear/Fully Connected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA for dimensionality reduction before t-SNE\n",
    "t0 = time.time()  # Start timer\n",
    "\n",
    "# Perform t-SNE\n",
    "n_components_tsne = 2  # t-SNE components (2D for visualization)\n",
    "n_iter_tsne = 4000  # Number of iterations for t-SNE optimization\n",
    "tsne = TSNE(n_components=n_components_tsne, random_state=0, n_iter=n_iter_tsne)\n",
    "\n",
    "\n",
    "activations_fc1_tsne = tsne.fit_transform(activations_first_linear)\n",
    "t1 = time.time()  # Stop timer\n",
    "\n",
    "print(f\"t-SNE done! Time elapsed: {t1 - t0:.2f} seconds\")\n",
    "print(\"Shape of t-SNE 2D features:\", activations_fc1_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the labels for test dataset\n",
    "labels = [y for _, y in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_folder, f't_SNE_fc1_{str_dt}.png')\n",
    "plot_tsne(activations_fc1_tsne, labels, title=\"t-SNE visualization of VGG16 activation, Dense Layer 1\", filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA for dimensionality reduction before t-SNE\n",
    "t0 = time.time()  # Start timer\n",
    "\n",
    "# Perform t-SNE\n",
    "n_components_tsne = 2  # t-SNE components (2D for visualization)\n",
    "n_iter_tsne = 4000  # Number of iterations for t-SNE optimization\n",
    "tsne = TSNE(n_components=n_components_tsne, random_state=0, n_iter=n_iter_tsne)\n",
    "\n",
    "\n",
    "activations_fc2_tsne = tsne.fit_transform(activations_second_linear)\n",
    "t1 = time.time()  # Stop timer\n",
    "\n",
    "print(f\"t-SNE done! Time elapsed: {t1 - t0:.2f} seconds\")\n",
    "print(\"Shape of t-SNE 2D features:\", activations_fc2_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_folder, f't_SNE_fc2_{str_dt}.png')\n",
    "plot_tsne(activations_fc2_tsne, labels, title=\"t-SNE visualization of VGG16 activation, Dense Layer 2\", filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA for dimensionality reduction before t-SNE\n",
    "t0 = time.time()  # Start timer\n",
    "\n",
    "# Perform t-SNE\n",
    "n_components_tsne = 2  # t-SNE components (2D for visualization)\n",
    "n_iter_tsne = 4000  # Number of iterations for t-SNE optimization\n",
    "tsne = TSNE(n_components=n_components_tsne, random_state=0, n_iter=n_iter_tsne)\n",
    "\n",
    "\n",
    "activations_fc3_tsne = tsne.fit_transform(activations_third_linear)\n",
    "t1 = time.time()  # Stop timer\n",
    "\n",
    "print(f\"t-SNE done! Time elapsed: {t1 - t0:.2f} seconds\")\n",
    "print(\"Shape of t-SNE 2D features:\", activations_fc3_tsne.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(model_folder, f't_SNE_fc3_{str_dt}.png')\n",
    "plot_tsne(activations_fc3_tsne, labels, title=\"t-SNE visualization of ResNet50 activation, Output Layer\", filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml-block3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
